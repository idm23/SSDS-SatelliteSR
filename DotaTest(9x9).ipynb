{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DotaTest.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOark8u+RS1cP9YT+qproAM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2T09vJV8eBa","executionInfo":{"status":"ok","timestamp":1622172106195,"user_tz":240,"elapsed":15948,"user":{"displayName":"Ian Mackey","photoUrl":"","userId":"12206211692171117643"}},"outputId":"7dfb7907-44fb-4886-9b81-1df821458f2f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/SSDSResearchSP21"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/SSDSResearchSP21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzurqI1G9tIJ","executionInfo":{"status":"ok","timestamp":1622172117312,"user_tz":240,"elapsed":11125,"user":{"displayName":"Ian Mackey","photoUrl":"","userId":"12206211692171117643"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","import torch.utils.data\n","import torchvision\n","from torchvision import transforms\n","from PIL import Image\n","from skimage import feature\n","from itertools import permutations\n","\n","from models import Glow\n","import gc"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuk4mFYO-dSp","executionInfo":{"status":"ok","timestamp":1622172193713,"user_tz":240,"elapsed":76404,"user":{"displayName":"Ian Mackey","photoUrl":"","userId":"12206211692171117643"}}},"source":["root_dir = 'dotaDset_interpolate' # Set dataset to use\n","folderList = os.listdir(root_dir) # Get list of folders in Dataset currently in use"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka3xMfLY3mGf"},"source":["\"\"\"\n","meta_files = os.listdir('labelTxt')\n","n9_samples = []\n","n = 0\n","for metadata in meta_files:\n","  with open(os.path.join('labelTxt/', metadata), 'r') as f:\n","    try:\n","      gsd = float(f.readlines()[1][4:])\n","      if gsd <= 0.1:\n","        n9_samples.append([metadata[:-4], gsd])\n","    except:\n","      print(\"Failed to read GSD\")\n","    print(n)\n","    n += 1\n","\n","print(len(n9_samples))\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bsAtFRNE6vjS"},"source":["\"\"\"\n","DOTA_dir = 'dota_data/images'\n","randomCrop = transforms.RandomCrop(size = 128)\n","cropped_images = []\n","for img_name, gsd in n9_samples:\n","  hq_img = Image.open(os.path.join(DOTA_dir, img_name+ '.png')).convert('RGB')\n","  for i in range(10):\n","    cropped_images.append([randomCrop(hq_img), gsd])\n","\n","for img, _ in cropped_images:\n","  display(img) \n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DA7AYIBi9NQ9"},"source":["\"\"\"\n","random.shuffle(cropped_images)\n","origin_images = np.zeros((9, 9, 3, 128, 128), dtype = \"float32\") # Preallocate space for all the images to be used\n","\n","n8 = np.array([.1, .2])\n","n7 = np.array([.2, .4])\n","n6 = np.array([.4, .75])\n","n5 = np.array([.75, 1.2])\n","n4 = np.array([1.2, 2.5])\n","n3 = np.array([2.5, 4.5])\n","n2 = np.array([4.5, 9.0])\n","n1 = np.array([9.0, 20])\n","NIIRS_ranges = np.stack((n8, n7, n6, n5, n4, n3, n2, n1))\n","\n","up_transform = transforms.Resize(size = 128)\n","half_transform = transforms.Resize(size = 64)\n","\n","count = 0\n","for cropped_image, gsd in cropped_images[:9]:\n","  NIIRS_iter = 0\n","  min_dim = 0\n","  while(min_dim <= 0 and max_dim <= 0):\n","    cropped_image = up_transform(half_transform(cropped_image))\n","    gsd *= 2\n","    min_factor = 9.0/gsd\n","    max_factor = 20/gsd\n","    min_dim = round(128/max_factor)\n","    max_dim = round(128/min_factor)\n","  for nRange in NIIRS_ranges:\n","    if(NIIRS_iter == 7):\n","      origin_images[count][8] = np.transpose(np.asarray(cropped_image, dtype = \"float32\")/256, (2,0,1))\n","    \n","    min_factor = nRange[0]/gsd\n","    max_factor = nRange[1]/gsd\n","    min_dim = round(128/max_factor)\n","    max_dim = round(128/min_factor)\n","    print(min_factor)\n","    print(max_factor)\n","    size_im = random.randint(min_dim, max_dim)\n","    down_transform = transforms.Resize(size = size_im)\n","    downsampled_im = down_transform(cropped_image)\n","    transformed_im = up_transform(downsampled_im)\n","    origin_images[count][NIIRS_iter] = np.transpose(np.asarray(transformed_im, dtype = \"float32\")/256, (2,0,1))\n","    NIIRS_iter += 1\n","  count += 1\n","  \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2oHmGjw_Jt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622173313753,"user_tz":240,"elapsed":1107845,"user":{"displayName":"Ian Mackey","photoUrl":"","userId":"12206211692171117643"}},"outputId":"65a2188b-e945-419f-c4a9-002f4f3b1b5c"},"source":["complete_Folders = []\n","iter = 0\n","for folder_name in folderList:\n","  folder_path = os.path.join(root_dir, folder_name) # Get path to each folder\n","  imgs = os.listdir(folder_path) # Get names of all imgs in each folder\n","  if len(imgs) == 9: # Collect all folder paths with imgs at NIIRS score 1 through 9 in one list\n","        print(folder_path)\n","        complete_Folders.append(folder_path)\n","  #print(\"Number of folders passed through: \" + str(iter))\n","  #print(\"Number of complete folders: \"+ str(len(complete_Folders)))\n","  #print()\n","  iter += 1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["dotaDset_interpolate/P1139_0\n","dotaDset_interpolate/P1139_1\n","dotaDset_interpolate/P1139_2\n","dotaDset_interpolate/P1139_3\n","dotaDset_interpolate/P1139_4\n","dotaDset_interpolate/P1139_5\n","dotaDset_interpolate/P1139_6\n","dotaDset_interpolate/P1139_7\n","dotaDset_interpolate/P1139_8\n","dotaDset_interpolate/P1139_9\n","dotaDset_interpolate/P1818_0\n","dotaDset_interpolate/P1818_1\n","dotaDset_interpolate/P1818_2\n","dotaDset_interpolate/P1818_3\n","dotaDset_interpolate/P1818_4\n","dotaDset_interpolate/P1818_5\n","dotaDset_interpolate/P1818_6\n","dotaDset_interpolate/P1818_7\n","dotaDset_interpolate/P1818_8\n","dotaDset_interpolate/P1818_9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TvoCYffJ_hJT"},"source":["print(complete_Folders)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jykiWGr0AxuU"},"source":["random.shuffle(complete_Folders) # Shuffle the complete folders for sampling\n","samplingFolders = complete_Folders[:9] # Choose 9 different complete folders for visualization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CG67eM86CSFy"},"source":["origin_images = np.zeros((9, 9, 3, 128, 128), dtype = \"float32\") # Preallocate space for all the images to be used\n","folder_iter = 0\n","for folder_path in samplingFolders:\n","  for i in range(9):\n","    img = Image.open(os.path.join(folder_path, str(i+1) + '.jpg')) # Open each image\n","    img = np.asarray(img).astype('float32')/256 # Normalize each image\n","    img = np.transpose(img, (2, 0, 1)) # Swap dimensions for easier conversion to tensors later (W, H, C) -> (C, W, H)\n","    origin_images[folder_iter][i] = img # Save each image into larger array\n","  folder_iter += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8ZSu23MHazZ"},"source":["# Set the device and load the most recent network\n","device = 'cuda' if torch.cuda.is_available() and args.gpu_ids else 'cpu'\n","net = Glow(num_channels=128, num_levels=6, num_steps=8, mode='multi', cond_channels = 20)\n","net = net.to(device)\n","if device == 'cuda':\n","  net = torch.nn.DataParallel(net, [0])\n","\n","checkpoint = torch.load('ckpts/recent.pth.tar', map_location = torch.device(device))\n","state_dict = checkpoint['net']\n","try:\n","  net.load_state_dict(state_dict)\n","except:\n","  from collections import OrderedDict\n","  new_state_dict = OrderedDict()\n","  for k, v in state_dict.items():\n","    name = k[7:] # remove `module.`\n","    new_state_dict[name] = v\n","  net.load_state_dict(new_state_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zra8zPe0NdHk"},"source":["# Define function to create samples from the network\n","def sample(net, cond_img, device, sigma=0.6):\n","    B, C, W, H = cond_img.shape\n","    z = torch.randn((B, 3, W, H), dtype=torch.float32, device=device) * sigma\n","    x, _ = net(z, cond_img, reverse=True)\n","    x = torch.sigmoid(x)\n","\n","    return x\n","\n","# Define function that takes\n","def conditioned_img(image, x_NIIRS, cond_NIIRS):\n","  cond_img = np.transpose(image, (1, 2, 0))\n","  cond_img = np.dstack((cond_img, np.full((128,128), x_NIIRS/9, dtype = \"float32\")))\n","  cond_img = np.dstack((cond_img, np.full((128,128), cond_NIIRS/9, dtype = \"float32\")))\n","  cond_img = np.transpose(cond_img, (2, 0, 1))\n","  return cond_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM-25yE4HlP9","executionInfo":{"status":"ok","timestamp":1620655372417,"user_tz":240,"elapsed":138938,"user":{"displayName":"Ian Mackey","photoUrl":"","userId":"12206211692171117643"}},"outputId":"3e1872bd-304b-4b34-b171-673ec49f9609"},"source":["\"\"\"\n","gt_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","output_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","input_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","\n","for folder_iter in range(9):\n","  for i in range(9):\n","    gt_images[(9*folder_iter)+i] = torch.from_numpy(origin_images[folder_iter][i])\n","    input_images[(9*folder_iter)+i] = torch.from_numpy(origin_images[folder_iter][folder_iter])\n","    if i == folder_iter:\n","      output_images[(9*folder_iter)+i] = torch.from_numpy(origin_images[folder_iter][i])\n","    else:\n","      cond_img = torch.from_numpy(conditioned_img(origin_images[folder_iter][folder_iter], folder_iter, i))\n","      output_images[(9*folder_iter)+i] = sample(net, torch.unsqueeze(cond_img, 0), device)\n","\n","gt_concat = torchvision.utils.make_grid(gt_images, nrow=9, padding=2, pad_value=255)\n","input_concat = torchvision.utils.make_grid(input_images, nrow=9, padding=2, pad_value=255)\n","output_concat = torchvision.utils.make_grid(output_images, nrow=9, padding=2, pad_value=255)\n","\n","torchvision.utils.save_image(gt_concat, 'dotaTest_files/groundTruth.png')\n","torchvision.utils.save_image(input_concat, 'dotaTest_files/inputs.png')\n","torchvision.utils.save_image(output_concat, 'dotaTest_files/outputs.png')\n","\"\"\"\n","\n","output_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","for folder_iter in range(9):\n","  for i in range(9):\n","    if i == folder_iter:\n","      output_images[(9*i)+folder_iter] = torch.from_numpy(origin_images[folder_iter][folder_iter])\n","    else:\n","      cond_img = torch.from_numpy(conditioned_img(origin_images[folder_iter][folder_iter], i+1, folder_iter+1))\n","      out = sample(net, torch.unsqueeze(cond_img, 0), device)\n","      output_images[(9*i)+folder_iter] = out[0]\n","      #print(out[0])\n","#print(output_images.shape)\n","output_concat = torchvision.utils.make_grid(output_images, nrow=9, padding=2, pad_value=255)\n","torchvision.utils.save_image(output_concat, 'dotaTest_files/outputs.png')\n","del output_images\n","del output_concat\n","gc.collect()\n","\n","gt_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","for folder_iter in range(9):\n","  for i in range(9):\n","    gt_images[(9*i)+folder_iter] = torch.from_numpy(origin_images[folder_iter][i])\n","#print(gt_images.shape)\n","gt_concat = torchvision.utils.make_grid(gt_images, nrow=9, padding=2, pad_value=255)\n","torchvision.utils.save_image(gt_concat, 'dotaTest_files/groundTruth.png')\n","del gt_images\n","del gt_concat\n","gc.collect()\n","\n","input_images = torch.zeros(size = (81, 3, 128, 128), dtype = torch.float32)\n","for folder_iter in range(9):\n","  for i in range(9):\n","    input_images[(9*i)+folder_iter] = torch.from_numpy(origin_images[folder_iter][folder_iter])\n","input_concat = torchvision.utils.make_grid(input_images, nrow=9, padding=2, pad_value=255)\n","torchvision.utils.save_image(input_concat, 'dotaTest_files/inputs.png')\n","del input_images\n","del input_concat\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"hrjdmbNIDTaz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jo6yS9NTCLs9"},"source":[""],"execution_count":null,"outputs":[]}]}